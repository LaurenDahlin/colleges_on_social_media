
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{04\_tweet\_processing}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{how-to-process-tweets-for-text-analysis}{%
\section{How to Process Tweets for Text
Analysis}\label{how-to-process-tweets-for-text-analysis}}

    \hypertarget{objective-process-tweets-for-text-analysis-including-topic-modelling-and-wordcloud-visualization.-processing-includes-separating-words-within-hashtags-removing-entity-specific-words-stop-words-and-lemmatization.}{%
\subsubsection{Objective: Process Tweets for text analysis including
topic modelling and wordcloud visualization. Processing includes
separating words within hashtags, removing entity-specific words, stop
words, and
lemmatization.}\label{objective-process-tweets-for-text-analysis-including-topic-modelling-and-wordcloud-visualization.-processing-includes-separating-words-within-hashtags-removing-entity-specific-words-stop-words-and-lemmatization.}}

    In the previous step we obtained all tweets for our sample of colleges
and universities. Now, however, we must process the messy tweet text for
analysis. There are many different approaches to cleaning tweet data for
text analysis and topic modelling. For example, some authors combine
Tweets with the same hashtag into a single document. Upon inspection,
however, I decided that would not be helpful in the context of college
and university's social media use. This is because many universities use
hashtags that are just that college's name (too generic and frequent)
e.g. \#somecollege or specific university hashtags that are not used
frequently (e.g. \#somecollegehomecoming).

Additionally, some colleges use only a hashtag when mentioning the topic
I am most interested in for this work -- \#diversity, while others do
not use a hashtag, e.g. ``We support \#womenoncampus'' vs ``We support
women on campus.'' Because of this, I use the \texttt{wordsegment}
package to attempt to split multi-word hashtags. Note however that this
is not a perfect process when words overlap.

Finally, because this analysis is used in words used across colleges,
not just at a particular college, I remove words that are only used by
one college in the data. This helps with the problem mentioned above
where colleges hashtag their own names. Their own names will occur
frequently in the data, but not frequently enough to be removed by
standard text processing techniques, such as removing words that occur
in over a certain percentage of documents.

    \hypertarget{set-up}{%
\subsection{Set Up}\label{set-up}}

    \hypertarget{load-packages}{%
\subsubsection{Load packages}\label{load-packages}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{from} \PY{n+nn}{collections} \PY{k}{import} \PY{n}{defaultdict}
\end{Verbatim}


    The \texttt{preprocessor} package will remove emojis, mentions, and URLs
from tweets.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Note: There is a bug installing this package in windows that was fixed by a kind user on }
        \PY{c+c1}{\PYZsh{} Github. Windows users should download as follows}
        \PY{c+c1}{\PYZsh{} pip install git+git://github.com/iamRusty/preprocessor}
        \PY{k+kn}{import} \PY{n+nn}{preprocessor} \PY{k}{as} \PY{n+nn}{pre}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{import} \PY{n+nn}{re}\PY{o}{,} \PY{n+nn}{glob}\PY{o}{,} \PY{n+nn}{datetime}
\end{Verbatim}


    The \texttt{wordsegment} package splits multiword hashtags into
individual words.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kn}{from} \PY{n+nn}{wordsegment} \PY{k}{import} \PY{n}{load}\PY{p}{,} \PY{n}{segment}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Help with memory issues}
        \PY{k+kn}{import} \PY{n+nn}{gc}
\end{Verbatim}


    The \texttt{unicodedata} package helps to convert unicode to ascii.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kn}{import} \PY{n+nn}{unicodedata2}
\end{Verbatim}


    The \texttt{nltk} and \texttt{gensim} packages are used for stemming and
removing stop words.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Tools for LDA}
        \PY{k+kn}{import} \PY{n+nn}{gensim}
        \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{simple\PYZus{}preprocess}
        \PY{k+kn}{from} \PY{n+nn}{gensim}\PY{n+nn}{.}\PY{n+nn}{parsing}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{STOPWORDS}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem} \PY{k}{import} \PY{n}{WordNetLemmatizer}\PY{p}{,} \PY{n}{SnowballStemmer}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem}\PY{n+nn}{.}\PY{n+nn}{porter} \PY{k}{import} \PY{o}{*}
        \PY{k+kn}{import} \PY{n+nn}{nltk}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}laure\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}gensim\textbackslash{}utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize\_serial
  warnings.warn("detected Windows; aliasing chunkize to chunkize\_serial")

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{n}{nltk}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{wordnet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[nltk\_data] Downloading package wordnet to
[nltk\_data]     C:\textbackslash{}Users\textbackslash{}laure\textbackslash{}AppData\textbackslash{}Roaming\textbackslash{}nltk\_data{\ldots}
[nltk\_data]   Package wordnet is already up-to-date!

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}10}]:} True
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n}{nltk}\PY{o}{.}\PY{n}{download}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{stopwords}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[nltk\_data] Downloading package stopwords to
[nltk\_data]     C:\textbackslash{}Users\textbackslash{}laure\textbackslash{}AppData\textbackslash{}Roaming\textbackslash{}nltk\_data{\ldots}
[nltk\_data]   Package stopwords is already up-to-date!

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}11}]:} True
\end{Verbatim}
            
    \hypertarget{functions-to-clean-the-tweet-text}{%
\subsection{Functions to Clean the Tweet
Text}\label{functions-to-clean-the-tweet-text}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{} Test data }
         \PY{n}{twitter\PYZus{}files} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/twitter\PYZus{}data/*.json}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{test\PYZus{}file} \PY{o}{=} \PY{n}{twitter\PYZus{}files}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{test\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}json}\PY{p}{(}\PY{n}{test\PYZus{}file}\PY{p}{,} \PY{n}{encoding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{utf\PYZhy{}8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lines}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Test tweet}
         \PY{n}{test\PYZus{}tweet} \PY{o}{=} \PY{n}{test\PYZus{}df}\PY{o}{.}\PY{n}{tweet}\PY{p}{[}\PY{l+m+mi}{99}\PY{p}{]}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{test\PYZus{}tweet}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
\#UAHDiscoveryDay2017 Group Silver: Your first session is the Welcome! Join us in the Charger Union Theater for a quick welcome and rundown of your day on campus.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} Load segmentation dictionary}
         \PY{n}{load}\PY{p}{(}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Set options for Twitter processing \PYZhy{} remove url, mention, and emojis}
         \PY{n}{pre}\PY{o}{.}\PY{n}{set\PYZus{}options}\PY{p}{(}\PY{n}{pre}\PY{o}{.}\PY{n}{OPT}\PY{o}{.}\PY{n}{URL}\PY{p}{,} \PY{n}{pre}\PY{o}{.}\PY{n}{OPT}\PY{o}{.}\PY{n}{EMOJI}\PY{p}{,} \PY{n}{pre}\PY{o}{.}\PY{n}{OPT}\PY{o}{.}\PY{n}{MENTION}\PY{p}{,} \PY{n}{pre}\PY{o}{.}\PY{n}{OPT}\PY{o}{.}\PY{n}{SMILEY}\PY{p}{)}
\end{Verbatim}


    The following function will clean the hashtags, removing hashes and
numbers, as well as segmenting out individual words.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Hash fix \PYZhy{} will be called from within tweet cleaning function}
         \PY{k}{def} \PY{n+nf}{hash\PYZus{}fix}\PY{p}{(}\PY{n}{h}\PY{p}{)}\PY{p}{:}
             \PY{n}{h1} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[0\PYZhy{}9]+}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{h}\PY{p}{)}
             \PY{n}{h2} \PY{o}{=} \PY{n}{re}\PY{o}{.}\PY{n}{sub}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{h1}\PY{p}{)}
             \PY{n}{h3} \PY{o}{=} \PY{n}{segment}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{h2}\PY{p}{)}\PY{p}{)}
             \PY{n}{h4} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n+nb}{str}\PY{p}{,} \PY{n}{h3}\PY{p}{)}\PY{p}{)} 
             \PY{k}{return} \PY{n}{h4}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{hash\PYZus{}fix}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}UAHDiscoveryDay2017}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}15}]:} 'uah discovery day'
\end{Verbatim}
            
    Create a dictionary that contains each hash tag and the text it should
be replaced with.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} Inputs: dataframe with the tweets and the column with the hashtags}
         \PY{k}{def} \PY{n+nf}{hash\PYZus{}dict}\PY{p}{(}\PY{n}{df}\PY{p}{,}\PY{n}{hash\PYZus{}col}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Create a datafame of all hashtags in a column and their counts}
             \PY{c+c1}{\PYZsh{} Note: hashtags are in lists inside a cell e.g. [\PYZsh{}hash1, \PYZsh{}hash2] }
             \PY{n}{tag\PYZus{}counts} \PY{o}{=} \PY{n}{df}\PY{p}{[}\PY{n}{hash\PYZus{}col}\PY{p}{]}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{)}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{p}{)}
             \PY{n}{tag\PYZus{}counts} \PY{o}{=} \PY{n}{tag\PYZus{}counts}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
             \PY{n}{tag\PYZus{}counts}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{freq}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
             \PY{c+c1}{\PYZsh{} Remove numbers and segment multiple words using hash fix}
             \PY{n}{tag\PYZus{}counts} \PY{o}{=} \PY{n}{tag\PYZus{}counts}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{clean\PYZus{}tag} \PY{o}{=} \PY{n}{tag\PYZus{}counts}\PY{o}{.}\PY{n}{hash}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{hash\PYZus{}fix}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Create a dictionary of the hashtags and their clean strings}
             \PY{n}{tag\PYZus{}counts}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hash}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{tag\PYZus{}dict} \PY{o}{=} \PY{n}{tag\PYZus{}counts}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clean\PYZus{}tag}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}dict}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{tag\PYZus{}dict}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{tag\PYZus{}dict} \PY{o}{=} \PY{n}{hash\PYZus{}dict}\PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hashtags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{tag\PYZus{}dict}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} \{'\#chargernation': 'charger nation',
          '\#uahuntsville': 'ua huntsville',
          '\#uahopenhouse': 'uah open house',
          '\#gochargers': 'go chargers',
          '\#foundmyhome': 'found my home',
          '\#chargeon': 'charge on',
          '\#uah20': 'uah',
          '\#uahorientation': 'uah orientation',
          '\#awesome': 'awesome',
          '\#uah17': 'uah',
          '\#uah': 'uah',
          '\#uahadmittedstudentday': 'uah admitted student day',
          '\#uah18': 'uah',
          '\#uahchargerpreview': 'uah charger preview',
          '\#uahdiscoverydays': 'uah discovery days',
          '\#uahroadtrip': 'uah road trip',
          '\#uahtour': 'uah tour',
          '\#uahdiscoveryday2017': 'uah discovery day',
          '\#thisismyuah': 'this is my uah',
          '\#chargerpride': 'charger pride',
          '\#uah16': 'uah',
          '\#uahnso': 'uah n so',
          '\#uah19': 'uah',
          '\#uah2022': 'uah',
          '\#uah17facebook': 'uah facebook',
          '\#uahbasketballbash': 'uah basketball bash',
          '\#protip': 'pro tip',
          '\#ff': 'ff',
          '\#uahadmissions': 'uah admissions',
          '\#foundmyhome16': 'found my home',
          '\#universityofawesomeinhuntsville': 'university of awesome in huntsville',
          '\#uahol': 'ua hol',
          '\#myuah': 'my uah',
          '\#uah21': 'uah',
          '\#nomnoms': 'nom noms',
          '\#puckapalooza': 'pucka palooza',
          '\#uahvisit': 'uah visit',
          '\#uah2016': 'uah',
          '\#parentsessions': 'parent sessions',
          '\#swag': 'swag',
          '\#yourock': 'you rock',
          '\#college': 'college',
          '\#futurechargers': 'future chargers',
          '\#followfriday': 'follow friday',
          '\#uah22': 'uah',
          '\#uahsummerevents': 'uah summer events',
          '\#excited': 'excited',
          '\#tbt': 'tbt',
          '\#chargerpreview2018': 'charger preview',
          '\#teamrockets': 'team rockets',
          '\#ussrc': 'us src',
          '\#destinationchargernation': 'destination charger nation',
          '\#hunwx': 'hun wx',
          '\#swagbag': 'swag bag',
          '\#collegebound': 'collegebound',
          '\#spooky': 'spooky',
          '\#chargerblue': 'charger blue',
          '\#win': 'win',
          '\#pumped': 'pumped',
          '\#blueschools': 'blue schools',
          '\#huntsvillebound': 'huntsville bound',
          '\#orientation': 'orientation',
          '\#uahunstville': 'uah unst ville',
          '\#gsc': 'gsc',
          '\#battleofthebuffalo': 'battle of the buffalo',
          '\#alwx': 'al wx',
          '\#beautiful': 'beautiful',
          '\#huntsville': 'huntsville',
          '\#lunch': 'lunch',
          '\#nom': 'nom',
          '\#chargersat1': 'chargers at',
          '\#uahchargers': 'uah chargers',
          '\#parentsession': 'parent session',
          '\#cwl': 'cwl',
          '\#brightandearly': 'bright and early',
          '\#welcome': 'welcome',
          '\#runbabyrun': 'run baby run',
          '\#high5friday': 'high friday',
          '\#teambluebabies': 'team blue babies',
          '\#o2santa': 'o santa',
          '\#goblueandwhite': 'go blue and white',
          '\#perfectweather': 'perfect weather',
          '\#nature': 'nature',
          '\#asduah': 'as du ah',
          '\#alabama': 'alabama',
          '\#sopretty': 'so pretty',
          '\#winit': 'win it',
          '\#uahsaidyes': 'uah said yes',
          '\#ireallyshouldputachairontheroof': 'i really should put a chair on the roof',
          '\#takeuahhomefortheholidays': 'take uah home for the holidays',
          '\#winitweekend': 'win it weekend',
          '\#teamusa': 'team usa',
          '\#fcw': 'fcw',
          '\#uahtastytuesday': 'uah tasty tuesday',
          '\#rocketcity': 'rocket city',
          '\#pumpyouup': 'pump you up',
          '\#homecomingweek2011': 'homecoming week',
          '\#proud': 'proud',
          '\#gobigblue': 'go big blue',
          '\#classof2013': 'class of',
          '\#atx': 'atx',
          '\#education': 'education',
          '\#kyfbla': 'ky fbla',
          '\#repost': 'repost',
          '\#wilsonhall': 'wilson hall',
          '\#nso': 'n so',
          '\#iloveuah': 'i love uah',
          '\#sunset': 'sunset',
          '\#cbahotdogroast': 'cba hot dog roast',
          '\#chargerbound': 'charger bound'\}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{test\PYZus{}df}\PY{o}{.}\PY{n}{tweet}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{tag\PYZus{}dict}\PY{p}{,} \PY{n}{regex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:} 0       College can be scary, but getting here doesn’t{\ldots}
         1       We’re so excited to see everyone in the mornin{\ldots}
         2       Online registration is open for our 3 Discover{\ldots}
         3       If you’re interested in UAH and want to learn {\ldots}
         4       Yes! We will announce those dates a little clo{\ldots}
         5       We will be hosting 3 overnight visits this sem{\ldots}
         6       Interested in becoming a UAH Orientation Leade{\ldots}
         7       Another record-breaking year! https://www.uah{\ldots}
         8       Interested in UAH, but live too far away to vi{\ldots}
         9       We are looking forward to having tons of stude{\ldots}
         10      tbt Back when Charlie was just a high school s{\ldots}
         11      Charger Preview is 2 weeks from Saturday! Don'{\ldots}
         12      It's STILL not too late! https://twitter.com/U{\ldots}
         13      We just love our Rocket City! Huntsville was r{\ldots}
         14      Tomorrow is New Student Orientation Round ✋ Ge{\ldots}
         15      Charger Preview is July 21! This event is for {\ldots}
         16      The University of Alabama in Huntsville will b{\ldots}
         17      Tomorrow is our first Orientaion session of th{\ldots}
         18      Our undergraduate students get to do some cool{\ldots}
         19      For the third year in a row, UAH is ranked \#1 {\ldots}
         20      Make sure you're following us on Snapchat (uah{\ldots}
         21      UAH takes first place in the College/Universit{\ldots}
         22      Register for Orientation TODAY! Sessions are f{\ldots}
         23      It's finally Admitted Student Day! We are exci{\ldots}
         24      Wonderful! Can't wait to see the pictures you {\ldots}
         25      The weather for Admitted Student Day tomorrow {\ldots}
         26      UAH has two teams in the competition this year{\ldots}
         27      We can't wait for Admitted Student Day this Sa{\ldots}
         28      Sign up for Orientation! New Student Orientati{\ldots}
         29      Admitted Student Day is quickly approaching! A{\ldots}
                                       {\ldots}                        
         1949    Don't forget to check out Diving for Dollars s{\ldots}
         1950    Having a blast at \#UAHuntsville WOW week? Send{\ldots}
         1951    Move-in day is here!! Welcome to campus new Ch{\ldots}
         1952    Move-in day is here!! Welcome to campus new Ch{\ldots}
         1953    Kicking off the last Orientation! Welcome home{\ldots}
         1954    The scholarship application will be closed for{\ldots}
         1955    Scholarship applications for the 2011/2012 aca{\ldots}
         1956    \#UAHuntsville is up 18\% in 2011 Freshmen admis{\ldots}
         1957                           Move-in day is in 17 days!
         1958    RT @uahuntsville: R\&B singer @jasonderulo will{\ldots}
         1959    RT @uahuntsville: \#UAHuntsville is now on Goog{\ldots}
         1960    RT @uahuntsville: RT @SophieStrong: Im \#Excite{\ldots}
         1961    RT @waaytv: UAHuntsville Researchers to Improv{\ldots}
         1962    RT @tomspacecamp: RT @SpaceCampSara: You can e{\ldots}
         1963    Congrats to Rakeem Fuller for winning the \$500{\ldots}
         1964    RT @name\_is\_mucho: @UAHuntsville i got orienta{\ldots}
         1965    RT @cweis1992: @UAHuntsville i am and can not {\ldots}
         1966    RT @haylibager: Orientation tomorrow @UAHuntsv{\ldots}
         1967    RT @uahuntsville: Looking forward to having yo{\ldots}
         1968    Good morning new Chargers! We are ready for or{\ldots}
         1969    Orientation starts up again tomorrow! Welcome {\ldots}
         1970    Orientation 4 is underway! Welcome to the family!
         1971    Make sure to look for your button match while {\ldots}
         1972    Yes, we actually hold a record for the longest{\ldots}
         1973    We haven't yet perfected time travel in scient{\ldots}
         1974    You heard right, there is no charge or tuition{\ldots}
         1975    New students at Orientation, we really think y{\ldots}
         1976    Excited to see you all out for Orientation thi{\ldots}
         1977    Don't forget to check in on Foursquare if you {\ldots}
         1978    Orientation Number 2 is underway! Remember to {\ldots}
         Name: tweet, Length: 1979, dtype: object
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Function to clean tweet dataframe}
         \PY{k}{def} \PY{n+nf}{clean\PYZus{}tweets}\PY{p}{(}\PY{n}{df}\PY{p}{,} \PY{n}{drop\PYZus{}cols}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Create dictionary of \PYZdq{}cleaned\PYZdq{} hashtags}
             \PY{n}{tag\PYZus{}dict} \PY{o}{=} \PY{n}{hash\PYZus{}dict}\PY{p}{(}\PY{n}{df}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hashtags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Create column with clean tweets}
             \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{clean\PYZus{}tw} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{tweet}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{pre}\PY{o}{.}\PY{n}{clean}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Replace hashtags in clean tweets using dictionary}
             \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{clean\PYZus{}tw} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{clean\PYZus{}tw}\PY{o}{.}\PY{n}{replace}\PY{p}{(}\PY{n}{tag\PYZus{}dict}\PY{p}{,} \PY{n}{regex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Drop unused cols}
             \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{drop\PYZus{}cols}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{k}{return} \PY{n}{df}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} Example: Clean DF}
         \PY{c+c1}{\PYZsh{} Columns to drop}
         \PY{n}{drop\PYZus{}cols} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{created\PYZus{}at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gif\PYZus{}thumb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}reply\PYZus{}to}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{location}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mentions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{quote\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{quote\PYZus{}url}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{replies}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{timezone}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{new\PYZus{}df} \PY{o}{=} \PY{n}{clean\PYZus{}tweets}\PY{p}{(}\PY{n}{test\PYZus{}df}\PY{p}{,} \PY{n}{drop\PYZus{}cols}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{new\PYZus{}df}\PY{o}{.}\PY{n}{clean\PYZus{}tw}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} 0    College can be scary, but getting here doesn’t{\ldots}
         1    We’re so excited to see everyone in the mornin{\ldots}
         2    Online registration is open for our 3 Discover{\ldots}
         3    If you’re interested in UAH and want to learn {\ldots}
         4    Yes! We will announce those dates a little clo{\ldots}
         Name: clean\_tw, dtype: object
\end{Verbatim}
            
    \hypertarget{concatenate-twitter-data-from-json-files-into-single-dataframe}{%
\subsection{Concatenate Twitter Data from JSON Files into Single
Dataframe}\label{concatenate-twitter-data-from-json-files-into-single-dataframe}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} Get list of JSON files}
         \PY{n}{twitter\PYZus{}files} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/twitter\PYZus{}data/*.json}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{twitter\PYZus{}files}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
11.29

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{} Note: Having memory issues so I\PYZsq{}m breaking the file list into chunks }
         \PY{c+c1}{\PYZsh{} If you have files you need to run individually, set no\PYZus{}tricks = False}
         \PY{c+c1}{\PYZsh{} and put your \PYZdq{}tricky\PYZdq{} file into line 9}
         \PY{k}{def} \PY{n+nf}{appended\PYZus{}files}\PY{p}{(}\PY{n}{file\PYZus{}list}\PY{p}{,} \PY{n}{chunk\PYZus{}num}\PY{p}{,} \PY{n}{drop\PYZus{}cols}\PY{p}{,} \PY{n}{no\PYZus{}tricks}\PY{p}{)}\PY{p}{:}
             \PY{n}{appended\PYZus{}data} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{file} \PY{o+ow}{in} \PY{n}{file\PYZus{}list}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} Print file and time for debugging purposes}
                 \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{findall}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[0\PYZhy{}9].*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}json}\PY{p}{(}\PY{n}{file}\PY{p}{,} \PY{n}{encoding}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{utf\PYZhy{}8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{lines}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Code is hanging on this large file \PYZhy{} skip cleaning it}
                 \PY{k}{if} \PY{p}{(}\PY{n}{no\PYZus{}tricks} \PY{o}{==} \PY{k+kc}{True} \PY{o+ow}{and} \PY{n+nb}{str}\PY{p}{(}\PY{n}{re}\PY{o}{.}\PY{n}{findall}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{[0\PYZhy{}9].*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{217156\PYZus{}main.json}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}
                     \PY{k}{continue}
                 \PY{c+c1}{\PYZsh{} Clean dataframe using function from previous step}
                 \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{clean\PYZus{}tweets}\PY{p}{(}\PY{n}{tw\PYZus{}df}\PY{p}{,} \PY{n}{drop\PYZus{}cols}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Extract ID number}
                 \PY{n}{id\PYZus{}num} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)} \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{isdigit}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Add ID number to DF}
                 \PY{n}{tw\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{=}\PY{n}{id\PYZus{}num}
                 \PY{c+c1}{\PYZsh{} Add indicator for main page vs admissions page}
                 \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{main}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{:}
                     \PY{n}{tw\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{main\PYZus{}page}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{=}\PY{l+m+mi}{1}
                 \PY{k}{elif} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{adm}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{:}
                     \PY{n}{tw\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{main\PYZus{}page}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{=}\PY{l+m+mi}{0}
                 \PY{k}{else}\PY{p}{:}
                     \PY{k}{pass}
                 \PY{c+c1}{\PYZsh{} Reset index}
                 \PY{n}{tw\PYZus{}df}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{drop}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Add to list to append}
                 \PY{k}{if} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{file\PYZus{}list}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                     \PY{n}{appended\PYZus{}data}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tw\PYZus{}df}\PY{p}{)}
                 \PY{c+c1}{\PYZsh{} Run garbage collection}
                 \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Concatenate files in list}
             \PY{k}{if} \PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{file\PYZus{}list}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{appended\PYZus{}data}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Save concatenated files to pickle}
             \PY{n}{tw\PYZus{}df}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/twitter\PYZus{}data/pickle/concat\PYZus{}tw\PYZus{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{chunk\PYZus{}num}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Delete object}
             \PY{k}{del} \PY{n}{tw\PYZus{}df}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{} Create a function to create \PYZdq{}chunks\PYZdq{} of the list of files}
         \PY{c+c1}{\PYZsh{} https://stackoverflow.com/questions/434287/what\PYZhy{}is\PYZhy{}the\PYZhy{}most\PYZhy{}pythonic\PYZhy{}way\PYZhy{}to\PYZhy{}iterate\PYZhy{}over\PYZhy{}a\PYZhy{}list\PYZhy{}in\PYZhy{}chunks}
         \PY{k}{def} \PY{n+nf}{chunker}\PY{p}{(}\PY{n}{seq}\PY{p}{,} \PY{n}{size}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{p}{(}\PY{n}{seq}\PY{p}{[}\PY{n}{pos}\PY{p}{:}\PY{n}{pos} \PY{o}{+} \PY{n}{size}\PY{p}{]} \PY{k}{for} \PY{n}{pos} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{seq}\PY{p}{)}\PY{p}{,} \PY{n}{size}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} Columns to drop}
         \PY{n}{drop\PYZus{}cols} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{created\PYZus{}at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gif\PYZus{}thumb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}quote\PYZus{}status}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is\PYZus{}reply\PYZus{}to}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{location}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mentions}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{name}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{place}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{quote\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{quote\PYZus{}url}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
             \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{replies}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{retweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tags}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{time}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{timezone}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} 15474
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{} Clean and append files in chunks}
         \PY{c+c1}{\PYZsh{} Note: Computer hanging up \PYZhy{} do this in chunks}
         \PY{c+c1}{\PYZsh{} If process hangs and you need to restart at a chunk, change twitter files to }
         \PY{c+c1}{\PYZsh{} twitter\PYZus{}files[start\PYZus{}file\PYZus{}number:] and i to the number of the chunk}
         \PY{c+c1}{\PYZsh{} Example: i=5 twitter\PYZus{}files[250:] will start at the 5th chunk for chunks of 50}
         \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{1}
         \PY{k}{for} \PY{n}{group} \PY{o+ow}{in} \PY{n}{chunker}\PY{p}{(}\PY{n}{twitter\PYZus{}files}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{:}
             \PY{n}{chunk\PYZus{}str} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZpc{}02d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{p}{)}
             \PY{n}{appended\PYZus{}files}\PY{p}{(}\PY{n}{group}\PY{p}{,} \PY{n}{chunk\PYZus{}str}\PY{p}{,} \PY{n}{drop\PYZus{}cols}\PY{p}{,} \PY{n}{no\PYZus{}tricks}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{c+c1}{\PYZsh{} Run garbage collection}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
             \PY{n}{i} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
216852\_main.json - 2018-12-15 09:40:05.825548
216931\_adm.json - 2018-12-15 09:40:28.776106
216931\_main.json - 2018-12-15 09:40:30.202339
217156\_adm.json - 2018-12-15 09:40:38.210865
217156\_main.json - 2018-12-15 09:40:39.469350
217165\_adm.json - 2018-12-15 09:40:40.678950
217165\_main.json - 2018-12-15 09:40:52.651845
217235\_main.json - 2018-12-15 09:41:07.459032
217420\_adm.json - 2018-12-15 09:41:38.809539
217420\_main.json - 2018-12-15 09:41:41.815739
217484\_main.json - 2018-12-15 09:41:48.265548
217493\_main.json - 2018-12-15 09:41:48.668729
217518\_adm.json - 2018-12-15 09:41:54.018408
217518\_main.json - 2018-12-15 09:41:54.489165
217536\_main.json - 2018-12-15 09:42:10.210251
217688\_main.json - 2018-12-15 09:42:16.796390
217749\_main.json - 2018-12-15 09:42:30.173260
217819\_adm.json - 2018-12-15 09:42:33.692169
217819\_main.json - 2018-12-15 09:42:52.600567
217864\_main.json - 2018-12-15 09:43:31.124285
217882\_adm.json - 2018-12-15 09:43:37.932327
217882\_main.json - 2018-12-15 09:43:39.734309
218061\_main.json - 2018-12-15 09:43:50.672358
218070\_main.json - 2018-12-15 09:43:56.320167
218229\_main.json - 2018-12-15 09:44:21.422759
218238\_main.json - 2018-12-15 09:44:23.831659
218663\_adm.json - 2018-12-15 09:44:24.743224
218663\_main.json - 2018-12-15 09:44:29.319711
218724\_adm.json - 2018-12-15 09:45:06.466742
218724\_main.json - 2018-12-15 09:45:19.635414
218733\_main.json - 2018-12-15 09:45:32.519776
218742\_main.json - 2018-12-15 09:45:40.859098
218964\_main.json - 2018-12-15 09:46:10.300855
218973\_adm.json - 2018-12-15 09:46:17.633433
218973\_main.json - 2018-12-15 09:46:21.075775
219000\_main.json - 2018-12-15 09:46:22.884960
219356\_main.json - 2018-12-15 09:46:41.858735
219471\_adm.json - 2018-12-15 09:46:43.140073
219471\_main.json - 2018-12-15 09:46:43.949916
219602\_adm.json - 2018-12-15 09:46:46.399623
219602\_main.json - 2018-12-15 09:46:47.731211
219709\_main.json - 2018-12-15 09:46:53.823935
219718\_main.json - 2018-12-15 09:46:55.613410
219806\_main.json - 2018-12-15 09:46:58.828086
219976\_adm.json - 2018-12-15 09:47:01.089869
219976\_main.json - 2018-12-15 09:47:04.018030
220075\_adm.json - 2018-12-15 09:47:10.294415
220075\_main.json - 2018-12-15 09:47:11.553270
220516\_main.json - 2018-12-15 09:47:16.557988
220613\_main.json - 2018-12-15 09:47:20.564185
220862\_adm.json - 2018-12-15 09:47:30.263817
220862\_main.json - 2018-12-15 09:47:32.700185
220978\_adm.json - 2018-12-15 09:47:38.098459
220978\_main.json - 2018-12-15 09:47:39.230326
221351\_main.json - 2018-12-15 09:48:13.144293
221740\_main.json - 2018-12-15 09:48:19.625061
221759\_adm.json - 2018-12-15 09:48:41.536906
221759\_main.json - 2018-12-15 09:48:45.342488
221768\_adm.json - 2018-12-15 09:49:01.970390
221768\_main.json - 2018-12-15 09:49:05.706181
221838\_adm.json - 2018-12-15 09:49:07.107002
221838\_main.json - 2018-12-15 09:49:09.157963
221847\_adm.json - 2018-12-15 09:49:17.953691
221847\_main.json - 2018-12-15 09:49:21.085349
221892\_main.json - 2018-12-15 09:49:35.286218
221953\_main.json - 2018-12-15 09:49:46.631095
221971\_main.json - 2018-12-15 09:49:53.441247
221999\_main.json - 2018-12-15 09:49:57.513660
222178\_adm.json - 2018-12-15 09:50:52.088997
222178\_main.json - 2018-12-15 09:50:53.285300
222831\_adm.json - 2018-12-15 09:50:57.116234
222831\_main.json - 2018-12-15 09:50:58.183151
223232\_adm.json - 2018-12-15 09:51:01.418895
223232\_main.json - 2018-12-15 09:51:09.853514
224004\_adm.json - 2018-12-15 09:51:52.161166
224004\_main.json - 2018-12-15 09:51:55.415822
224147\_adm.json - 2018-12-15 09:52:09.558103
224147\_main.json - 2018-12-15 09:52:11.657230
224226\_adm.json - 2018-12-15 09:52:32.857693
224226\_main.json - 2018-12-15 09:52:34.273650
224554\_main.json - 2018-12-15 09:52:43.503674
225247\_main.json - 2018-12-15 09:52:50.032140
225399\_main.json - 2018-12-15 09:52:56.771453
225432\_adm.json - 2018-12-15 09:53:03.197551
225432\_main.json - 2018-12-15 09:53:03.981996
225627\_adm.json - 2018-12-15 09:53:12.191919
225627\_main.json - 2018-12-15 09:53:14.612136
226091\_main.json - 2018-12-15 09:53:27.223761
226231\_main.json - 2018-12-15 09:53:33.381082
226471\_main.json - 2018-12-15 09:53:38.001829
226833\_adm.json - 2018-12-15 09:53:41.013412
226833\_main.json - 2018-12-15 09:53:45.205185
227331\_main.json - 2018-12-15 09:54:00.387464
227368\_main.json - 2018-12-15 09:54:03.518559
227526\_adm.json - 2018-12-15 09:54:12.517065
227526\_main.json - 2018-12-15 09:54:14.796946
227757\_main.json - 2018-12-15 09:54:22.982959
227881\_main.json - 2018-12-15 09:55:20.961040
228149\_adm.json - 2018-12-15 09:55:43.484404
228149\_main.json - 2018-12-15 09:55:45.954028
228246\_main.json - 2018-12-15 09:56:31.383441
228343\_main.json - 2018-12-15 09:56:58.264767
228431\_main.json - 2018-12-15 09:57:24.509846
228459\_adm.json - 2018-12-15 09:57:29.051135
228459\_main.json - 2018-12-15 09:57:30.988475
228529\_main.json - 2018-12-15 09:59:11.307468
228705\_adm.json - 2018-12-15 09:59:16.448504
228705\_main.json - 2018-12-15 09:59:19.852077
228723\_adm.json - 2018-12-15 09:59:30.304444
228723\_main.json - 2018-12-15 09:59:36.821327
228778\_adm.json - 2018-12-15 10:00:26.844340
228778\_main.json - 2018-12-15 10:00:29.671582
228787\_main.json - 2018-12-15 10:01:10.273169
228802\_adm.json - 2018-12-15 10:01:36.549744
228802\_main.json - 2018-12-15 10:01:44.611273
228875\_adm.json - 2018-12-15 10:01:49.842294
228875\_main.json - 2018-12-15 10:01:51.309125
229063\_adm.json - 2018-12-15 10:02:03.103598
229063\_main.json - 2018-12-15 10:02:03.685216
229115\_adm.json - 2018-12-15 10:02:05.985510
229115\_main.json - 2018-12-15 10:02:13.788750
229179\_main.json - 2018-12-15 10:03:08.847627
229267\_adm.json - 2018-12-15 10:03:12.161921
229267\_main.json - 2018-12-15 10:03:23.928212
229780\_main.json - 2018-12-15 10:03:45.723907
229814\_main.json - 2018-12-15 10:03:47.731581
230038\_adm.json - 2018-12-15 10:03:59.269025
230038\_main.json - 2018-12-15 10:03:59.887932
230603\_main.json - 2018-12-15 10:04:07.191657
230728\_main.json - 2018-12-15 10:04:16.713584
230737\_main.json - 2018-12-15 10:04:25.890197
230764\_adm.json - 2018-12-15 10:04:28.952842
230764\_main.json - 2018-12-15 10:04:30.145091
230782\_main.json - 2018-12-15 10:05:49.270622
230807\_main.json - 2018-12-15 10:05:54.431944
230995\_main.json - 2018-12-15 10:06:11.783807
231174\_main.json - 2018-12-15 10:06:19.241286
231624\_adm.json - 2018-12-15 10:06:33.534370
231624\_main.json - 2018-12-15 10:06:35.593883
231712\_adm.json - 2018-12-15 10:06:54.796048
231712\_main.json - 2018-12-15 10:06:56.774414
232423\_main.json - 2018-12-15 10:06:59.819222
232557\_adm.json - 2018-12-15 10:07:21.564342
232557\_main.json - 2018-12-15 10:07:30.499124
232566\_adm.json - 2018-12-15 10:07:48.340644
232566\_main.json - 2018-12-15 10:07:56.968301
232609\_adm.json - 2018-12-15 10:08:19.601780
232609\_main.json - 2018-12-15 10:08:20.996833
232681\_main.json - 2018-12-15 10:08:39.745853
232706\_adm.json - 2018-12-15 10:08:45.316772
232706\_main.json - 2018-12-15 10:08:51.703668
232982\_main.json - 2018-12-15 10:08:55.286009
233277\_adm.json - 2018-12-15 10:09:00.126319
233277\_main.json - 2018-12-15 10:09:01.126804
233374\_adm.json - 2018-12-15 10:09:06.858417
233374\_main.json - 2018-12-15 10:09:09.269080
233921\_adm.json - 2018-12-15 10:09:26.192664
233921\_main.json - 2018-12-15 10:09:28.882497
234030\_adm.json - 2018-12-15 10:09:48.818476
234030\_main.json - 2018-12-15 10:09:50.941061
234076\_adm.json - 2018-12-15 10:10:31.291687
234076\_main.json - 2018-12-15 10:10:31.714912
234155\_adm.json - 2018-12-15 10:11:09.647369
234155\_main.json - 2018-12-15 10:11:10.181678
234207\_adm.json - 2018-12-15 10:11:20.440426
234207\_main.json - 2018-12-15 10:11:22.349236
234827\_main.json - 2018-12-15 10:11:35.709595
235097\_main.json - 2018-12-15 10:11:41.472088
235167\_main.json - 2018-12-15 10:11:45.552659
235316\_adm.json - 2018-12-15 10:11:48.158019
235316\_main.json - 2018-12-15 10:11:49.516514
236230\_adm.json - 2018-12-15 10:12:01.498055
236230\_main.json - 2018-12-15 10:12:03.746305
236328\_main.json - 2018-12-15 10:12:13.659014
236577\_main.json - 2018-12-15 10:12:50.127751
236595\_adm.json - 2018-12-15 10:13:06.267144
236595\_main.json - 2018-12-15 10:13:07.227192
236948\_adm.json - 2018-12-15 10:13:16.941654
236948\_main.json - 2018-12-15 10:13:17.909074
237011\_main.json - 2018-12-15 10:13:53.188303
237057\_main.json - 2018-12-15 10:14:38.634305
237066\_main.json - 2018-12-15 10:14:43.997505
237330\_main.json - 2018-12-15 10:14:48.558478
237367\_main.json - 2018-12-15 10:14:50.770827
237525\_adm.json - 2018-12-15 10:15:01.573148
237525\_main.json - 2018-12-15 10:15:03.818168
237792\_main.json - 2018-12-15 10:15:12.842286
237899\_main.json - 2018-12-15 10:15:16.160757
237932\_main.json - 2018-12-15 10:15:19.255441
238193\_main.json - 2018-12-15 10:15:24.732266
238430\_adm.json - 2018-12-15 10:15:27.750687
238430\_main.json - 2018-12-15 10:15:28.744925
238458\_main.json - 2018-12-15 10:15:33.303112
238476\_main.json - 2018-12-15 10:15:46.258073
238616\_main.json - 2018-12-15 10:15:51.865389
238980\_main.json - 2018-12-15 10:16:02.273826
239080\_main.json - 2018-12-15 10:16:07.817503
239105\_adm.json - 2018-12-15 10:16:15.112221
239105\_main.json - 2018-12-15 10:16:22.676174
239318\_main.json - 2018-12-15 10:17:51.789181
240107\_main.json - 2018-12-15 10:17:59.693522
240189\_adm.json - 2018-12-15 10:18:18.476516
240189\_main.json - 2018-12-15 10:18:24.699190
240268\_adm.json - 2018-12-15 10:18:29.324248
240268\_main.json - 2018-12-15 10:18:33.068975
240277\_adm.json - 2018-12-15 10:18:50.139031
240277\_main.json - 2018-12-15 10:18:52.282026
240329\_adm.json - 2018-12-15 10:19:18.694392
240329\_main.json - 2018-12-15 10:19:19.252992
240365\_adm.json - 2018-12-15 10:19:21.954444
240365\_main.json - 2018-12-15 10:19:29.001280
240374\_adm.json - 2018-12-15 10:19:48.420129
240374\_main.json - 2018-12-15 10:19:51.099373
240417\_adm.json - 2018-12-15 10:19:54.878363
240417\_main.json - 2018-12-15 10:19:57.579096
240444\_adm.json - 2018-12-15 10:20:01.222918
240444\_main.json - 2018-12-15 10:20:04.818923
240462\_main.json - 2018-12-15 10:23:38.100349
240471\_main.json - 2018-12-15 10:23:49.683924
240480\_main.json - 2018-12-15 10:23:58.358429
240727\_main.json - 2018-12-15 10:24:03.396915
243744\_adm.json - 2018-12-15 10:24:07.490239
243744\_main.json - 2018-12-15 10:24:08.359617
243780\_adm.json - 2018-12-15 10:24:45.493916
243780\_main.json - 2018-12-15 10:24:48.281308
366711\_main.json - 2018-12-15 10:26:19.436469
450933\_adm.json - 2018-12-15 10:26:26.106177
450933\_main.json - 2018-12-15 10:26:32.805968

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{c+c1}{\PYZsh{} For some reason one .json file will not process. Tried stripping non\PYZhy{}ascii characters.}
        \PY{c+c1}{\PYZsh{} It will have to be left out for now. }
        \PY{c+c1}{\PYZsh{} Run one large, pesky file separately \PYZhy{} 217156\PYZus{}main.json}
        \PY{c+c1}{\PYZsh{}appended\PYZus{}files([\PYZsq{}../data/twitter\PYZus{}data/217156\PYZus{}fixed\PYZus{}main.json\PYZsq{}], \PYZsq{}217156\PYZsq{}, drop\PYZus{}cols, no\PYZus{}tricks=False)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
217156\_fixed\_main.json - 2018-12-17 19:33:11.123541

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} Concatenate only the cleaned tweets from the cleaned pkl files}
         \PY{c+c1}{\PYZsh{} Saves memory space}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{pkl\PYZus{}files} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/twitter\PYZus{}data/pickle/concat\PYZus{}tw\PYZus{}*.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{append\PYZus{}tweets} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{file} \PY{o+ow}{in} \PY{n}{pkl\PYZus{}files}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{)}
             \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}pickle}\PY{p}{(}\PY{n}{file}\PY{p}{)}
             \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{tw\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clean\PYZus{}tw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
             \PY{n}{append\PYZus{}tweets}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tw\PYZus{}df}\PY{p}{)}
             \PY{k}{del} \PY{n}{tw\PYZus{}df}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{tw\PYZus{}final} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{append\PYZus{}tweets}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{tw\PYZus{}final}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/clean\PYZus{}tweets\PYZus{}full.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_01.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_02.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_03.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_04.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_05.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_06.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_07.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_08.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_09.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_10.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_11.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_12.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_13.pkl

    \end{Verbatim}

    \hypertarget{process-text-and-create-diversity-tweet-dataset}{%
\subsection{Process Text and Create Diversity Tweet
Dataset}\label{process-text-and-create-diversity-tweet-dataset}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Load the pickled data}
         \PY{n}{tw\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/clean\PYZus{}tweets\PYZus{}full.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    Unfortunately there are still non-ascii characters - let's remove them
now.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k}{def} \PY{n+nf}{convert\PYZus{}unicode}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{unicodedata2}\PY{o}{.}\PY{n}{normalize}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{NFKD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{text}\PY{p}{)}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ascii}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ignore}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{tw\PYZus{}data} \PY{o}{=} \PY{n}{tw\PYZus{}data}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{clean\PYZus{}tw} \PY{o}{=} \PY{n}{tw\PYZus{}data}\PY{o}{.}\PY{n}{clean\PYZus{}tw}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{convert\PYZus{}unicode}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    Preprocessing code comes from link below. Though, note the author forgot
to define stemmer.

https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{} Stem and remove stop words}
         \PY{n}{stemmer} \PY{o}{=} \PY{n}{SnowballStemmer}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{english}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ignore\PYZus{}stopwords}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{lemmatize\PYZus{}stemming}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{stemmer}\PY{o}{.}\PY{n}{stem}\PY{p}{(}\PY{n}{WordNetLemmatizer}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{lemmatize}\PY{p}{(}\PY{n}{text}\PY{p}{,} \PY{n}{pos}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{v}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
         
         \PY{k}{def} \PY{n+nf}{preprocess}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
             \PY{n}{result} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{gensim}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{simple\PYZus{}preprocess}\PY{p}{(}\PY{n}{text}\PY{p}{)}\PY{p}{:}
                 \PY{k}{if} \PY{n}{token} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{gensim}\PY{o}{.}\PY{n}{parsing}\PY{o}{.}\PY{n}{preprocessing}\PY{o}{.}\PY{n}{STOPWORDS} \PY{o+ow}{and} \PY{n+nb}{len}\PY{p}{(}\PY{n}{token}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{3}\PY{p}{:}
                     \PY{n}{result}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{lemmatize\PYZus{}stemming}\PY{p}{(}\PY{n}{token}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{result}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{tw\PYZus{}data} \PY{o}{=} \PY{n}{tw\PYZus{}data}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{words} \PY{o}{=} \PY{n}{tw\PYZus{}data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{clean\PYZus{}tw}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{map}\PY{p}{(}\PY{n}{preprocess}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} 44
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} Save a copy in case you need to reload}
         \PY{n}{tw\PYZus{}data}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/pre\PYZus{}processed.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{n}{tw\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/pre\PYZus{}processed.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} Create a copy of the data with only the processed data, id, and school id}
         \PY{n}{tw\PYZus{}words} \PY{o}{=} \PY{n}{tw\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         \PY{k}{del} \PY{n}{tw\PYZus{}data}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} 7
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{12345}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Create a column with random number \PYZhy{} will be used to subset the data}
         \PY{n}{tw\PYZus{}words} \PY{o}{=} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{rand\PYZus{}int} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{99}\PY{p}{,} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \hypertarget{remove-words-used-by-only-one-college}{%
\subsection{Remove words used by only one
college}\label{remove-words-used-by-only-one-college}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} Remove words used by only one school \PYZhy{} this will help to filter out school names and mascots}
         \PY{c+c1}{\PYZsh{} This is tricky again because of memory issues}
         \PY{c+c1}{\PYZsh{} Stack the words in chunks \PYZhy{} remove words that are used by multiple schools}
         \PY{c+c1}{\PYZsh{} Final result will be list of words used only by one school}
         
         \PY{c+c1}{\PYZsh{} Must have list of words in tweet in column called \PYZsq{}words\PYZsq{}}
         \PY{k}{def} \PY{n+nf}{single\PYZus{}sch\PYZus{}words}\PY{p}{(}\PY{n}{df}\PY{p}{)}\PY{p}{:}
         
             \PY{c+c1}{\PYZsh{} Stack the data \PYZhy{} create row for each word in list}
             \PY{n}{df} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{set\PYZus{}index}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
             \PY{n}{df}\PY{o}{.}\PY{n}{sort\PYZus{}index}\PY{p}{(}\PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{s} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{stack}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{n}{level}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{drop}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{s}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}
             \PY{n}{tw\PYZus{}data\PYZus{}stacked} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{s}\PY{p}{)}
             \PY{k}{del} \PY{n}{s}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Keep only unique instances of each word and school}
             \PY{n}{tw\PYZus{}data\PYZus{}stacked} \PY{o}{=} \PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{drop\PYZus{}duplicates}\PY{p}{(}\PY{n}{subset}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{keep}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Create count of number of colleges that use each word}
             \PY{n}{num\PYZus{}colleges} \PY{o}{=} \PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{agg}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
             \PY{n}{num\PYZus{}colleges}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}
             \PY{n}{num\PYZus{}colleges}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
             \PY{n}{num\PYZus{}colleges}\PY{o}{.}\PY{n}{rename}\PY{p}{(}\PY{n}{index}\PY{o}{=}\PY{n+nb}{str}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{school\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{tw\PYZus{}data\PYZus{}stacked} \PY{o}{=} \PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{num\PYZus{}colleges}\PY{p}{,}\PY{n}{on}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{del} \PY{n}{num\PYZus{}colleges}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Keep words used by only one college}
             \PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
             \PY{n}{tw\PYZus{}data\PYZus{}stacked} \PY{o}{=} \PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{school\PYZus{}count} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
             \PY{n}{tw\PYZus{}data\PYZus{}stacked}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{school\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
             
             \PY{k}{return} \PY{n}{tw\PYZus{}data\PYZus{}stacked}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{} Datasets of single use words for random chunks of the data}
         \PY{c+c1}{\PYZsh{} Example using 1\PYZpc{} of data}
         \PY{n}{single\PYZus{}words1} \PY{o}{=} \PY{n}{single\PYZus{}sch\PYZus{}words}\PY{p}{(}\PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{rand\PYZus{}int} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{single\PYZus{}words1}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}36}]:}           ipeds\_id  rand\_int       word
         id                                     
         93773412    183239         0    orvieto
         789579480   210669         0     ascent
         836384863   209056         0      mcdow
         908199047   164580         0  nextworth
         922205934   228875         0     buccan
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{c+c1}{\PYZsh{} Get single words from random samples of 3\PYZpc{} of data at a time then append}
         \PY{n}{append\PYZus{}single} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{102}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
             \PY{n}{j} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{2}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Start at: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ End at: }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{j}\PY{p}{)} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ \PYZhy{} }\PY{l+s+s1}{\PYZsq{}} \PY{o}{+} \PY{n+nb}{str}\PY{p}{(}\PY{n}{datetime}\PY{o}{.}\PY{n}{datetime}\PY{o}{.}\PY{n}{now}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)} 
             \PY{n}{df} \PY{o}{=} \PY{n}{single\PYZus{}sch\PYZus{}words}\PY{p}{(}\PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{(}\PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{rand\PYZus{}int} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{i}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{rand\PYZus{}int} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{n}{j}\PY{p}{)}\PY{p}{]}\PY{p}{)}
             \PY{n}{append\PYZus{}single}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{df}\PY{p}{)}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Start at: 0 End at: 2 - 2018-12-18 13:30:07.326287
Start at: 3 End at: 5 - 2018-12-18 13:31:59.735666
Start at: 6 End at: 8 - 2018-12-18 13:33:41.026483
Start at: 9 End at: 11 - 2018-12-18 13:35:33.474505
Start at: 12 End at: 14 - 2018-12-18 13:37:22.630545
Start at: 15 End at: 17 - 2018-12-18 13:39:05.125096
Start at: 18 End at: 20 - 2018-12-18 13:40:41.995758
Start at: 21 End at: 23 - 2018-12-18 13:42:21.380203
Start at: 24 End at: 26 - 2018-12-18 13:43:59.934431
Start at: 27 End at: 29 - 2018-12-18 13:45:41.805603
Start at: 30 End at: 32 - 2018-12-18 13:47:18.330342
Start at: 33 End at: 35 - 2018-12-18 13:48:58.463605
Start at: 36 End at: 38 - 2018-12-18 13:50:38.119495
Start at: 39 End at: 41 - 2018-12-18 13:52:28.329703
Start at: 42 End at: 44 - 2018-12-18 13:54:18.916873
Start at: 45 End at: 47 - 2018-12-18 13:56:03.736739
Start at: 48 End at: 50 - 2018-12-18 13:57:48.435560
Start at: 51 End at: 53 - 2018-12-18 13:59:37.243402
Start at: 54 End at: 56 - 2018-12-18 14:01:30.655424
Start at: 57 End at: 59 - 2018-12-18 14:03:15.413516
Start at: 60 End at: 62 - 2018-12-18 14:05:07.227998
Start at: 63 End at: 65 - 2018-12-18 14:06:52.656103
Start at: 66 End at: 68 - 2018-12-18 14:08:40.924504
Start at: 69 End at: 71 - 2018-12-18 14:10:27.186727
Start at: 72 End at: 74 - 2018-12-18 14:12:03.697649
Start at: 75 End at: 77 - 2018-12-18 14:13:35.829157
Start at: 78 End at: 80 - 2018-12-18 14:15:11.689331
Start at: 81 End at: 83 - 2018-12-18 14:17:02.079004
Start at: 84 End at: 86 - 2018-12-18 14:18:34.542096
Start at: 87 End at: 89 - 2018-12-18 14:20:26.194343
Start at: 90 End at: 92 - 2018-12-18 14:22:11.956262
Start at: 93 End at: 95 - 2018-12-18 14:23:54.153736
Start at: 96 End at: 98 - 2018-12-18 14:25:38.045927
Start at: 99 End at: 101 - 2018-12-18 14:27:21.715944

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{c+c1}{\PYZsh{} Combine the three sets of single words, see if any are repeats}
         \PY{n}{single\PYZus{}words\PYZus{}all} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{append\PYZus{}single}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} Save in case of crash}
         \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/single\PYZus{}words.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{single\PYZus{}words\PYZus{}all} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/single\PYZus{}words.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} On further inspection, many of the single words look like mentions that weren\PYZsq{}t removed with the Twitter preprocessor.}
         \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:}   ipeds\_id  rand\_int        word
         0   152080         1      markup
         1   164580         2   zachrarki
         2   209056         0       mcdow
         3   210669         2    shemelya
         4   218742         1        dsc\_
         5   173300         2  gooseberri
         6   209056         2      rouari
         7   218742         2       liisa
         8   218742         2   salosaari
         9   218742         2    jasinski
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} On the combined dataset of random samples, count the number of schools that use each word}
         \PY{n}{num\PYZus{}colleges} \PY{o}{=} \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{agg}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{o}{.}\PY{n}{nunique}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{n}{num\PYZus{}colleges}\PY{o}{.}\PY{n}{name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{word}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{num\PYZus{}colleges}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
         \PY{n}{num\PYZus{}colleges}\PY{o}{.}\PY{n}{rename}\PY{p}{(}\PY{n}{index}\PY{o}{=}\PY{n+nb}{str}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{school\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{single\PYZus{}words\PYZus{}all} \PY{o}{=} \PY{n}{num\PYZus{}colleges}
         \PY{n}{single\PYZus{}words\PYZus{}all} \PY{o}{=} \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{school\PYZus{}count} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:}                school\_count
         word                       
         a\_\_schedul                1
         a\_\_www                    1
         a\_clever\_alia             1
         a\_dzi                     1
         a\_feiii                   1
         a\_garza\_duh               1
         a\_n\_g                     1
         a\_scot\_                   1
         a\_suarez                  1
         a\_wizzzl                  1
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{single\PYZus{}words\PYZus{}all} \PY{o}{=} \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{reset\PYZus{}index}\PY{p}{(}\PY{p}{)}
         \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}17}]:} Index(['word', 'school\_count'], dtype='object')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n}{single\PYZus{}word\PYZus{}list} \PY{o}{=} \PY{n}{single\PYZus{}words\PYZus{}all}\PY{o}{.}\PY{n}{word}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{n}{single\PYZus{}word\PYZus{}list}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} ['a\_\_schedul',
          'a\_\_www',
          'a\_clever\_alia',
          'a\_dzi',
          'a\_feiii',
          'a\_garza\_duh',
          'a\_n\_g',
          'a\_scot\_',
          'a\_suarez',
          'a\_wizzzl']
\end{Verbatim}
            
    Remove words used by only one college from the dataset.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rand\PYZus{}int}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{k}{def} \PY{n+nf}{remove\PYZus{}single}\PY{p}{(}\PY{n}{old\PYZus{}list}\PY{p}{,} \PY{n}{not\PYZus{}list}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{p}{[}\PY{n}{x} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n}{old\PYZus{}list} \PY{k}{if} \PY{n}{x} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{not\PYZus{}list}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{c+c1}{\PYZsh{} This takes a very very very long time to run}
         \PY{c+c1}{\PYZsh{} Did not have time to finish running, sadly}
         \PY{n}{tw\PYZus{}words} \PY{o}{=} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{words} \PY{o}{=} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{words}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{remove\PYZus{}single}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{single\PYZus{}word\PYZus{}list}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------

        KeyboardInterrupt                         Traceback (most recent call last)

        <ipython-input-29-a4786b178c97> in <module>
    ----> 1 tw\_words = tw\_words.assign(words = tw\_words.words.apply(lambda x: remove\_single(x, single\_word\_list)))
    

        \textasciitilde{}\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}pandas\textbackslash{}core\textbackslash{}series.py in apply(self, func, convert\_dtype, args, **kwds)
       3192             else:
       3193                 values = self.astype(object).values
    -> 3194                 mapped = lib.map\_infer(values, f, convert=convert\_dtype)
       3195 
       3196         if len(mapped) and isinstance(mapped[0], Series):
    

        pandas/\_libs/src\textbackslash{}inference.pyx in pandas.\_libs.lib.map\_infer()
    

        <ipython-input-29-a4786b178c97> in <lambda>(x)
    ----> 1 tw\_words = tw\_words.assign(words = tw\_words.words.apply(lambda x: remove\_single(x, single\_word\_list)))
    

        <ipython-input-27-eb913ead9e07> in remove\_single(old\_list, not\_list)
          1 def remove\_single(old\_list, not\_list):
    ----> 2     return [x for x in old\_list if x not in not\_list]
    

        <ipython-input-27-eb913ead9e07> in <listcomp>(.0)
          1 def remove\_single(old\_list, not\_list):
    ----> 2     return [x for x in old\_list if x not in not\_list]
    

        KeyboardInterrupt: 

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor} }]:} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/clean\PYZus{}tweets\PYZus{}final.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \hypertarget{diversity-analysis-dataset}{%
\subsection{Diversity Analysis
Dataset}\label{diversity-analysis-dataset}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{tw\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/pre\PYZus{}processed.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{tw\PYZus{}words} \PY{o}{=} \PY{n}{tw\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         \PY{k}{del} \PY{n}{tw\PYZus{}data}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}12}]:} 7
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{tw\PYZus{}words}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}13}]:}                     id                                              words
         0  1053360850965315584                  [uabhomecom, favorit, time, year]
         1  1052568776414298117  [admit, incom, freshman, class, congrat, check{\ldots}
         2  1021825776553996288  [growth, record, enrol, build, transit, commut{\ldots}
         3  1016333689486233600  [hope, have, great, summer, admit, freshmen, t{\ldots}
         4  1006548327687745536  [recent, admit, freshman, congrat, help, check{\ldots}
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{pkl\PYZus{}files} \PY{o}{=} \PY{n}{glob}\PY{o}{.}\PY{n}{glob}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/twitter\PYZus{}data/pickle/concat\PYZus{}tw\PYZus{}*.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{append\PYZus{}tweets} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{k}{for} \PY{n}{file} \PY{o+ow}{in} \PY{n}{pkl\PYZus{}files}\PY{p}{:}
             \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n}{file}\PY{p}{)}\PY{p}{)}
             \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}pickle}\PY{p}{(}\PY{n}{file}\PY{p}{)}
             \PY{n}{tw\PYZus{}df} \PY{o}{=} \PY{n}{tw\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ipeds\PYZus{}id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tweet}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{likes\PYZus{}count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{photos}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
             \PY{n}{append\PYZus{}tweets}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tw\PYZus{}df}\PY{p}{)}
             \PY{k}{del} \PY{n}{tw\PYZus{}df}
             \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{diversity\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{n}{append\PYZus{}tweets}\PY{p}{,} \PY{n}{ignore\PYZus{}index}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_01.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_02.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_03.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_04.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_05.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_06.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_07.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_08.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_09.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_10.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_11.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_12.pkl
../data/twitter\_data/pickle\textbackslash{}concat\_tw\_13.pkl

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{diversity\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{merge}\PY{p}{(}\PY{n}{diversity\PYZus{}df}\PY{p}{,} \PY{n}{tw\PYZus{}words}\PY{p}{,} \PY{n}{on}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{id}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{del} \PY{n}{tw\PYZus{}words}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} 14
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k}{def} \PY{n+nf}{list\PYZus{}contains}\PY{p}{(}\PY{n}{cell\PYZus{}list}\PY{p}{,} \PY{n}{word\PYZus{}list}\PY{p}{)}\PY{p}{:}
             \PY{k}{if} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{cell\PYZus{}list} \PY{k}{if} \PY{n}{i} \PY{o+ow}{in} \PY{n}{word\PYZus{}list}\PY{p}{]}\PY{p}{:}
                 \PY{k}{return} \PY{k+kc}{True}
             \PY{k}{else}\PY{p}{:}
                 \PY{k}{return} \PY{k+kc}{False}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{list\PYZus{}contains}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{list\PYZus{}contains}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
True
False

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} Identify diversity\PYZhy{}related tweets}
         \PY{n}{div\PYZus{}words} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{divers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{multicultur}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{diversity\PYZus{}df} \PY{o}{=} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{diversity\PYZus{}flag} \PY{o}{=} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{words}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{list\PYZus{}contains}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{div\PYZus{}words}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} Identify race\PYZhy{}related tweets}
         \PY{n}{race\PYZus{}words} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{african}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{asian}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hispan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{latino}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{latina}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{diversity\PYZus{}df} \PY{o}{=} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{race\PYZus{}flag} \PY{o}{=} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{words}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{list\PYZus{}contains}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{race\PYZus{}words}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} Identify gender\PYZhy{}related tweets}
         \PY{n}{gender\PYZus{}words} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{woman}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{women}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gender}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{diversity\PYZus{}df} \PY{o}{=} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{assign}\PY{p}{(}\PY{n}{gender\PYZus{}flag} \PY{o}{=} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{words}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{list\PYZus{}contains}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{gender\PYZus{}words}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{gender\PYZus{}flag}\PY{o}{==}\PY{k+kc}{True}\PY{p}{]}\PY{o}{.}\PY{n}{tweet}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} 425    The 2013 Fall Leadership Conference is underwa{\ldots}
         528    Happy Memorial Day, friends! Thank you to all {\ldots}
         874    RT @uabnews: They did it! The UAB Women's Bask{\ldots}
         878    Women's Basketball Invitational Championship G{\ldots}
         933    2 new women's sports at UAB!!!  http://fb.me/x{\ldots}
         Name: tweet, dtype: object
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}40}]:} 0
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:} Index(['ipeds\_id', 'id', 'date', 'tweet', 'likes\_count', 'photos', 'words',
                'diversity\_flag', 'race\_flag', 'gender\_flag'],
               dtype='object')
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{n+nb}{len}\PY{p}{(}\PY{n}{diversity\PYZus{}df}\PY{o}{.}\PY{n}{index}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}30}]:} 5907245
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{} Save data in chunks}
         \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{l+m+mi}{1000000}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/diversity\PYZus{}full\PYZus{}01.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{l+m+mi}{1000001}\PY{p}{:}\PY{l+m+mi}{2000000}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/diversity\PYZus{}full\PYZus{}02.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{l+m+mi}{2000001}\PY{p}{:}\PY{l+m+mi}{3000000}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/diversity\PYZus{}full\PYZus{}03.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{l+m+mi}{3000001}\PY{p}{:}\PY{l+m+mi}{4000000}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/diversity\PYZus{}full\PYZus{}04.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{l+m+mi}{4000001}\PY{p}{:}\PY{l+m+mi}{5000000}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/diversity\PYZus{}full\PYZus{}05.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{gc}\PY{o}{.}\PY{n}{collect}\PY{p}{(}\PY{p}{)}
         \PY{n}{diversity\PYZus{}df}\PY{p}{[}\PY{l+m+mi}{5000001}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{to\PYZus{}pickle}\PY{p}{(}\PY{n}{path}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{../data/clean/diversity\PYZus{}full\PYZus{}06.pkl}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}



    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
